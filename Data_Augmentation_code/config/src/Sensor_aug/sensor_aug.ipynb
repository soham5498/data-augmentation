{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63c443c",
   "metadata": {},
   "source": [
    "## Overview of the below code\n",
    "\n",
    "The code provides a comprehensive framework for sensor data augmentation, enabling the application of various augmentation techniques to enhance the dataset used for machine learning models. It consists of two main components: the `SensorAugmentation` class and the `process_sensor_data` function.\n",
    "\n",
    "### 1. `SensorAugmentation` Class\n",
    "\n",
    "#### Purpose\n",
    "The `SensorAugmentation` class encapsulates various techniques for augmenting sensor data. These techniques include adding noise, jittering, scaling, generating random curves, and permuting segments of the data.\n",
    "\n",
    "#### Methods\n",
    "\n",
    "- **`__init__(self, config)`**: Initializes the class with a configuration dictionary containing parameters for each augmentation technique.\n",
    "- **`add_gaussian_noise(self, data, mean=0, std=0.01)`**: Adds Gaussian noise to the data, clipping values to stay within the original data range.\n",
    "- **`add_uniform_noise(self, data, low=-0.01, high=0.01)`**: Adds uniform noise to the data, with values clipped to the original range.\n",
    "- **`DA_Jitter(self, data, sigma=0.05)`**: Applies jittering by adding random Gaussian noise to the data.\n",
    "- **`DA_Scaling(self, data, sigma=0.1)`**: Scales the data by multiplying it with random scaling factors.\n",
    "- **`GenerateRandomCurves(self, data, sigma=0.2, knot=4)`**: Generates and adds random curves to the data using cubic spline interpolation.\n",
    "- **`DA_Permutation(self, data, nPerm=3, minSegLength=10, noise_factor=0.1)`**: Permutes segments of the data, adding noise to each segment.\n",
    "- **`augment_data(self, data, techniques)`**: Applies specified augmentation techniques to the data based on the configuration provided.\n",
    "- **`save_augmented_data(self, augmented_data, base_filename, original_df, output_path)`**: Saves the augmented data to CSV files.\n",
    "\n",
    "### 2. `process_sensor_data` Function\n",
    "\n",
    "#### Purpose\n",
    "The `process_sensor_data` function orchestrates the entire process of loading sensor data, applying augmentation techniques, and saving the augmented data.\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "1. **Configuration Validation**: Ensures that the necessary configuration parameters are present.\n",
    "2. **Loading Data**: Reads sensor data from a specified CSV file into a DataFrame.\n",
    "3. **Data Augmentation**: Creates an instance of `SensorAugmentation` and applies the specified augmentation techniques to the data.\n",
    "4. **Saving Augmented Data**: Saves the augmented data to CSV files in the specified output directory.\n",
    "5. **Error Handling**: Includes comprehensive error handling for various potential issues such as file not found, permission errors, empty data files, and parsing errors.\n",
    "\n",
    "### Error Handling\n",
    "The `process_sensor_data` function includes robust error handling for various exceptions:\n",
    "- **PermissionError**: Ensures file paths are correct and required permissions are granted.\n",
    "- **FileNotFoundError**: Ensures the sensor data file exists at the specified path.\n",
    "- **EmptyDataError**: Handles cases where the data file is empty.\n",
    "- **ParserError**: Handles errors in parsing the data file.\n",
    "- **General Exceptions**: Catches any other exceptions that might occur during the process.\n",
    "\n",
    "### Summary\n",
    "This framework provides a flexible and efficient way to augment sensor data for machine learning models. The `SensorAugmentation` class offers multiple techniques for enhancing data, while the `process_sensor_data` function streamlines the process of loading, augmenting, and saving the data, with comprehensive error handling to ensure robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a008e64-d6fb-450d-991b-a9e1efef63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# References:\n",
    "# https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data/blob/master/Example_DataAugmentation_TimeseriesData.py\n",
    "\n",
    "class SensorAugmentation:\n",
    "    \"\"\"\n",
    "    A class to perform various sensor data augmentation techniques.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration for sensor data augmentation techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def add_gaussian_noise(self, data, mean=0, std=0.01):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise to the data.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            mean (float, optional): Mean of the Gaussian noise. Default is 0.\n",
    "            std (float, optional): Standard deviation of the Gaussian noise. Default is 0.01.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Data with added Gaussian noise.\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(mean, std, data.shape)\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        noisy_X = data + noise\n",
    "        noisy_X = np.clip(noisy_X, min_vals, max_vals)  # Clip values within the range of original data\n",
    "        return noisy_X\n",
    "\n",
    "    def add_uniform_noise(self, data, low=-0.01, high=0.01):\n",
    "        \"\"\"\n",
    "        Add uniform noise to the data.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            low (float, optional): Lower bound of the uniform noise. Default is -0.01.\n",
    "            high (float, optional): Upper bound of the uniform noise. Default is 0.01.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Data with added uniform noise.\n",
    "        \"\"\"\n",
    "        noise = np.random.uniform(low, high, data.shape)\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        noisy_X = data + noise\n",
    "        noisy_X = np.clip(noisy_X, min_vals, max_vals)\n",
    "        return noisy_X\n",
    "\n",
    "    def DA_Jitter(self, data, sigma=0.05):\n",
    "        \"\"\"\n",
    "        Apply jittering to the data by adding random Gaussian noise.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            sigma (float, optional): Standard deviation of the Gaussian noise. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Jittered data.\n",
    "        \"\"\"\n",
    "        myNoise = np.random.normal(loc=0, scale=sigma, size=data.shape)\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        noisy_X = data + myNoise\n",
    "        noisy_X = np.clip(noisy_X, min_vals, max_vals)\n",
    "        return noisy_X\n",
    "\n",
    "    def DA_Scaling(self, data, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Apply scaling to the data by multiplying with random scaling factors.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            sigma (float, optional): Standard deviation of the scaling factors. Default is 0.1.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Scaled data.\n",
    "        \"\"\"\n",
    "        scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1, data.shape[1]))\n",
    "        myNoise = np.matmul(np.ones((data.shape[0], 1)), scalingFactor)\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        noisy_X = data * myNoise\n",
    "        noisy_X = np.clip(noisy_X, min_vals, max_vals)\n",
    "        return noisy_X\n",
    "\n",
    "    def GenerateRandomCurves(self, data, sigma=0.2, knot=4):\n",
    "        \"\"\"\n",
    "        Generate random curves and add them to the data.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            sigma (float, optional): Standard deviation of the random curves. Default is 0.2.\n",
    "            knot (int, optional): Number of knots for the cubic spline. Default is 4.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Data with added random curves.\n",
    "        \"\"\"\n",
    "        x_range = np.arange(data.shape[0])\n",
    "        curves = np.zeros_like(data)\n",
    "        for i in range(data.shape[1]):\n",
    "            xx = np.linspace(0, data.shape[0] - 1, knot + 2)\n",
    "            yy = np.random.normal(loc=1.0, scale=sigma, size=knot + 2)\n",
    "            cs = CubicSpline(xx, yy)\n",
    "            curves[:, i] = cs(x_range)\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        noisy_X = data * curves\n",
    "        noisy_X = np.clip(noisy_X, min_vals, max_vals)\n",
    "        return noisy_X\n",
    "\n",
    "    def DA_Permutation(self, data, nPerm=3, minSegLength=10, noise_factor=0.1):\n",
    "        \"\"\"\n",
    "        Permute segments of the data.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            nPerm (int, optional): Number of permutations. Default is 4.\n",
    "            minSegLength (int, optional): Minimum segment length. Default is 10.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Data with permuted segments.\n",
    "        \"\"\"\n",
    "        X_new = np.zeros_like(data)\n",
    "        idx = np.random.permutation(nPerm)\n",
    "\n",
    "        bWhile = True\n",
    "        while bWhile:\n",
    "            segs = np.zeros(nPerm + 1, dtype=int)\n",
    "            segs[1:-1] = np.sort(np.random.randint(minSegLength, data.shape[0] - minSegLength, nPerm - 1))\n",
    "            segs[-1] = data.shape[0]\n",
    "            if np.min(segs[1:] - segs[:-1]) >= minSegLength:\n",
    "                bWhile = False\n",
    "\n",
    "        pp = 0\n",
    "        for ii in range(nPerm):\n",
    "            start_idx = segs[idx[ii]]\n",
    "            end_idx = segs[idx[ii] + 1] if (idx[ii] + 1) < len(segs) else data.shape[0]\n",
    "            x_temp = data[start_idx:end_idx, :].copy()\n",
    "            noise = np.random.normal(0, noise_factor, x_temp.shape)\n",
    "            x_temp += noise\n",
    "            X_new[pp:pp + len(x_temp), :] = x_temp\n",
    "            pp += len(x_temp)\n",
    "\n",
    "        # Ensure values stay within the original value range\n",
    "        min_vals = np.min(data, axis=0)\n",
    "        max_vals = np.max(data, axis=0)\n",
    "        X_new = np.clip(X_new, min_vals, max_vals)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    def augment_data(self, data, techniques):\n",
    "        \"\"\"\n",
    "        Apply specified augmentation techniques to the data.\n",
    "\n",
    "        Args:\n",
    "            data (ndarray): Input data.\n",
    "            techniques (dict): Dictionary of augmentation techniques and their parameters.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of augmented data with technique names as keys.\n",
    "        \"\"\"\n",
    "        augmented_data = {}\n",
    "        for technique, params in techniques.items():\n",
    "            if hasattr(self, technique) and params.get('enabled', False):\n",
    "                # Remove 'enabled' key before passing to the function\n",
    "                params = {k: v for k, v in params.items() if k != 'enabled'}\n",
    "                augmented_data[technique] = getattr(self, technique)(data, **params)\n",
    "        return augmented_data\n",
    "\n",
    "    def save_augmented_data(self, augmented_data, base_filename, orignal_df, output_path):\n",
    "        \"\"\"\n",
    "        Save the augmented data to CSV files.\n",
    "\n",
    "        Args:\n",
    "            augmented_data (dict): Dictionary of augmented data with technique names as keys.\n",
    "            base_filename (str): Base filename for the output files.\n",
    "            output_path (str): Path to the directory where the augmented files will be saved.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        for technique, data in augmented_data.items():\n",
    "            df = pd.DataFrame(data, columns=orignal_df.columns)\n",
    "            filename = os.path.join(output_path, f\"{base_filename}_{technique}.csv\")\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def process_sensor_data(config_data):\n",
    "    \"\"\"\n",
    "        Process sensor data according to the configuration.\n",
    "\n",
    "        Args:\n",
    "            config_data (dict): Configuration data containing paths and settings for sensor data augmentation.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            PermissionError: If there are issues with file permissions.\n",
    "            FileNotFoundError: If the sensor data file is not found at the specified path.\n",
    "            pd.errors.EmptyDataError: If the data file is empty.\n",
    "            pd.errors.ParserError: If there is an error parsing the data file.\n",
    "            Exception: For any other exceptions.\n",
    "        \"\"\"\n",
    "    try:\n",
    "        sensor_config = config_data[\"data_augmentation\"][\"sensor_augmentation\"]\n",
    "        data_path = sensor_config[\"script_path\"]\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Sensor data file not found at {data_path}\")\n",
    "\n",
    "        print(f\"Loading sensor data from {data_path}...\")\n",
    "        original_df = pd.read_csv(data_path)\n",
    "        data = original_df.to_numpy()\n",
    "        print(\"Data loaded successfully. Sample data:\")\n",
    "        augmenter = SensorAugmentation(sensor_config[\"techniques\"])\n",
    "        augmented_data = augmenter.augment_data(data, sensor_config[\"techniques\"])\n",
    "        output_path = sensor_config[\"output_path\"]\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        base_filename = os.path.splitext(os.path.basename(data_path))[0]\n",
    "        augmenter.save_augmented_data(augmented_data, base_filename, original_df, output_path)\n",
    "\n",
    "        print(\"Augmentation completed. Checking augmented files...\")\n",
    "\n",
    "        augmented_files = [f for f in os.listdir(output_path) if f.endswith('.csv') and os.path.isfile(os.path.join(output_path, f))]\n",
    "        for file in augmented_files:\n",
    "            file_path = os.path.join(output_path, file)\n",
    "            try:\n",
    "                augmented_df = pd.read_csv(file_path)\n",
    "                print(f\"Augmented Data from {file}:\")\n",
    "                print(augmented_df.head())\n",
    "                print(f\"Shape of augmented data from {file}: {augmented_df.shape}\")\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"UnicodeDecodeError: {e}. Could not read the file {file_path}. It might be encoded in a non-UTF-8 format.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Ensure the file path is correct and you have the required permissions.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}. Ensure the sensor data file exists at the specified path.\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f\"EmptyDataError: {e}. The data file is empty.\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"ParserError: {e}. Error parsing the data file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
