{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39609de8",
   "metadata": {},
   "source": [
    "Overview of the below code:\n",
    "\n",
    "This code consists of two main classes designed for handling text data, particularly in the context of Natural Language Processing (NLP). The two classes are `CustomDataset` and `TextPreprocessing`. Here's an overview of their functionalities and workflow:\n",
    "\n",
    "### 1. `CustomDataset` Class\n",
    "\n",
    "#### Purpose\n",
    "The `CustomDataset` class is designed to load and tokenize text data from a CSV file, preparing it for further use in machine learning models, especially those involving transformers like BERT.\n",
    "\n",
    "#### Attributes\n",
    "- `data`: A list of dictionaries containing tokenized text data and labels.\n",
    "- `tokenizer`: A `BertTokenizer` instance used for tokenizing the text data.\n",
    "- `max_length`: Maximum length of the tokenized sequences.\n",
    "- `df`: A DataFrame containing the data read from the CSV file.\n",
    "\n",
    "#### Methods\n",
    "- `__init__(self, file_path, tokenizer, max_length, text_columns)`: Initializes the dataset by reading the CSV file, validating columns, and tokenizing the text data.\n",
    "- `__len__(self)`: Returns the total number of samples in the dataset.\n",
    "- `__getitem__(self, idx)`: Retrieves the sample at the specified index, returning a dictionary containing tokenized text data.\n",
    "\n",
    "### 2. `TextPreprocessing` Class\n",
    "\n",
    "#### Purpose\n",
    "The `TextPreprocessing` class handles the preprocessing pipeline for text data, including tokenization, filtering, and various text preprocessing tasks such as lowercasing, contraction fixing, and URL removal.\n",
    "\n",
    "#### Attributes\n",
    "- `config`: Configuration dictionary containing file paths and settings for preprocessing.\n",
    "\n",
    "#### Methods\n",
    "- `__init__(self, config)`: Initializes the preprocessing object and validates the configuration.\n",
    "- `start_preprocessing(self)`: Executes the entire preprocessing pipeline, including tokenization, filtering, and text cleaning. Returns a DataFrame with the preprocessed text data.\n",
    "\n",
    "#### Workflow\n",
    "1. **Tokenize and Create Dataset**: Initializes a `BertTokenizer` and creates a `CustomDataset` instance using the provided file path and text columns.\n",
    "2. **Filter Dataset**: Filters the dataset to a specified number of samples.\n",
    "3. **Text Preprocessing**: Applies various text preprocessing steps:\n",
    "    - Converts text to lowercase.\n",
    "    - Fixes contractions using the `contractions` library.\n",
    "    - Removes URLs by replacing them with the word \"website\".\n",
    "    - Converts text back to tokenized format and updates the dataset.\n",
    "4. **Convert to DataFrame**: Converts the preprocessed text data back to a DataFrame.\n",
    "5. **Additional Preprocessing**: Further cleans the text data in the DataFrame by:\n",
    "    - Removing punctuation.\n",
    "    - Removing numbers.\n",
    "    - Removing newline and carriage return characters.\n",
    "6. **Handle NaN Values**: Replaces NaN values with empty strings to ensure data consistency.\n",
    "\n",
    "### Summary\n",
    "The `CustomDataset` class handles the loading and tokenization of text data, while the `TextPreprocessing` class performs a series of preprocessing steps to clean and prepare the text for further use. This setup is particularly useful for preparing text data for training NLP models using transformer architectures like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99396330-9576-404f-a86f-f76f672a82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bcae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "import contractions\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for loading and tokenizing text data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing data.\n",
    "        tokenizer (BertTokenizer): Tokenizer to be used for text tokenization.\n",
    "        max_length (int): Maximum length of the tokenized sequences.\n",
    "        text_columns (list): List of column names containing the text data.\n",
    "\n",
    "    Attributes:\n",
    "        data (list): List of dictionaries containing tokenized data and labels.\n",
    "        tokenizer (BertTokenizer): Tokenizer used for text tokenization.\n",
    "        max_length (int): Maximum length of the tokenized sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_length, text_columns):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Read the CSV file using pandas\n",
    "        print(file_path)\n",
    "        self.df = pd.read_csv(file_path, nrows=50)\n",
    "\n",
    "        # Ensure the CSV has the correct columns\n",
    "        for col in text_columns:\n",
    "            if col not in self.df.columns:\n",
    "                raise ValueError(f\"CSV file must contain '{col}' column.\")\n",
    "\n",
    "        # Process each row in the DataFrame\n",
    "        for _, row in self.df.iterrows():\n",
    "            combined_text = \" \".join([row[col] for col in text_columns])\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=True,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_special_tokens_mask=False,\n",
    "            )\n",
    "            self.data.append({\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing 'input_ids', 'attention_mask', and 'Sentiment' tensors.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class TextPreprocessing:\n",
    "    \"\"\"\n",
    "    Class for text preprocessing and visualization.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing file paths and other settings.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Configuration dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config  # Store the configuration\n",
    "\n",
    "        # Validate the \"text_data\" section contains \"file_path\" and \"text_columns\"\n",
    "        if \"file_path\" not in config[\"data_augmentation\"][\"text_augmentation\"] or \"text_columns\" not in config[\"data_augmentation\"][\"text_augmentation\"]:\n",
    "            raise KeyError(\"Configuration is missing 'file_path' or 'text_columns' in the 'text_augmentation' section.\")\n",
    "\n",
    "    def start_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Start the preprocessing pipeline.\n",
    "\n",
    "        Steps:\n",
    "            1. Tokenize and create dataset.\n",
    "            2. Filter dataset.\n",
    "            3. Apply text preprocessing (lowercase, contractions, URLs).\n",
    "            4. Convert preprocessed dataset to DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Preprocessed dataset as a DataFrame.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize and create dataset\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        dataset = CustomDataset(\n",
    "            os.path.join(self.config[\"data\"][\"base_input_path\"], self.config[\"data_augmentation\"][\"text_augmentation\"][\"file_path\"]),\n",
    "            tokenizer,\n",
    "            max_length=128,\n",
    "            text_columns=self.config[\"data_augmentation\"][\"text_augmentation\"][\"text_columns\"]\n",
    "        )\n",
    "        print(\"Dataset created\")\n",
    "\n",
    "        # Step 2: Filter dataset\n",
    "        dataset_filtered = dataset[:10]\n",
    "        print(\"Dataset filtered\")\n",
    "\n",
    "        # Step 3: Text preprocessing (lowercase, contractions, URLs)\n",
    "        for data in dataset_filtered:\n",
    "            text = data['input_ids'].tolist()\n",
    "            text = tokenizer.decode(text, skip_special_tokens=True)  # Remove special tokens\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text = contractions.fix(emoji.demojize(text))  # Fix contractions\n",
    "            text = text.replace(r\"https?://\\S+|www\\.\\S+\", \"website\")  # Remove URLs\n",
    "            encoded_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "            data['input_ids'] = torch.tensor(encoded_text)\n",
    "\n",
    "        print(\"Text preprocessing applied\")\n",
    "\n",
    "        # Step 4: Convert preprocessed dataset to DataFrame\n",
    "        df1 = pd.DataFrame({\n",
    "            'Sentence': [tokenizer.decode(data['input_ids'].tolist()) for data in dataset_filtered],\n",
    "        })\n",
    "\n",
    "        # Additional preprocessing steps\n",
    "        df1[\"Sentence\"] = df1[\"Sentence\"].str.lower()  # Convert text to lowercase\n",
    "        df1[\"Sentence\"] = df1[\"Sentence\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)  # Remove punctuation\n",
    "        df1[\"Sentence\"] = df1[\"Sentence\"].str.replace(r\"\\d+\", \"\", regex=True)  # Remove numbers\n",
    "        df1[\"Sentence\"] = df1[\"Sentence\"].str.replace(\"\\n\", \"\").replace(\"\\r\", \"\")  # Remove newline and carriage return characters\n",
    "\n",
    "        # Handle NaN values\n",
    "        df1.fillna(\"\", inplace=True)\n",
    "\n",
    "        return df1  # Return the preprocessed DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab951c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
