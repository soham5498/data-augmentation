{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b5cfa1",
   "metadata": {},
   "source": [
    "## Overview of the below code:\n",
    "\n",
    "Below code defines a Python class `TextAugmentation` that performs various text augmentation techniques using libraries such as `nltk`, `torch`, `transformers`, and `deep_translator`. Here's an overview of its components and functionalities:\n",
    "\n",
    "### Components and Functionalities\n",
    "\n",
    "1. **Class Initialization**:\n",
    "   - The `TextAugmentation` class is initialized with a configuration dictionary (`config`), which determines the augmentation techniques to be applied.\n",
    "   - It initializes tokenizers and models for T5 and BERT from the `transformers` library.\n",
    "\n",
    "2. **Text Cleaning**:\n",
    "   - `clean_text`: Cleans the input text by removing specific tokens and extra whitespace.\n",
    "\n",
    "3. **Text Augmentation Methods**:\n",
    "   - `synonym_replacement`: Replaces words in the text with their synonyms using WordNet.\n",
    "   - `get_synonyms`: Retrieves synonyms for a given word using WordNet.\n",
    "   - `add_noise`: Adds random noise to the text by replacing characters with random letters.\n",
    "   - `random_insertion`: Inserts random characters at random positions in the text.\n",
    "   - `random_deletion`: Deletes words from the text with a certain probability.\n",
    "   - `random_swap`: Swaps positions of random words in the text.\n",
    "   - `paraphrasing`: Paraphrases the text by replacing words with their synonyms.\n",
    "   - `style_transfer_nmt`: Performs style transfer using neural machine translation.\n",
    "   - `stochastic_text_generation`: Generates text stochastically by randomly selecting words from the input text.\n",
    "   - `masking`: Masks random words in the text.\n",
    "   - `correct_grammar`: Corrects grammar using a T5 model.\n",
    "   - `generation`: Generates text based on the input sentence using a T5 model.\n",
    "   - `combination`: Combines selected words from the input text into a new sentence.\n",
    "   - `conditional_text_generation`: Generates text conditionally based on the input using a T5 model.\n",
    "   - `bert_augmentation`: Augments text using a BERT model by masking and predicting words.\n",
    "   - `hierarchical_text_generation`: Generates text hierarchically by applying multiple augmentations.\n",
    "   - `back_translation`: Translates text to an intermediate language and back to the original language.\n",
    "\n",
    "4. **Applying Augmentations**:\n",
    "   - `start_augmentation_process`: Applies the selected augmentations to specified text columns in a DataFrame and returns the augmented DataFrame.\n",
    "\n",
    "### Detailed Method Descriptions\n",
    "\n",
    "- **Initialization**:\n",
    "  - Initializes T5 and BERT tokenizers and models from the `transformers` library.\n",
    "\n",
    "- **Text Cleaning**:\n",
    "  - **clean_text**: Removes specific tokens (`[CLS]`, `[SEP]`, `[PAD]`) and extra spaces.\n",
    "\n",
    "- **Augmentation Methods**:\n",
    "  - **synonym_replacement**: Uses WordNet to replace words with their synonyms.\n",
    "  - **get_synonyms**: Retrieves synonyms for a word from WordNet.\n",
    "  - **add_noise**: Introduces noise by replacing characters with random letters.\n",
    "  - **random_insertion**: Randomly inserts characters into the text.\n",
    "  - **random_deletion**: Randomly deletes words from the text.\n",
    "  - **random_swap**: Swaps random words in the text.\n",
    "  - **paraphrasing**: Paraphrases text by replacing words with their synonyms.\n",
    "  - **style_transfer_nmt**: Changes text style using neural machine translation.\n",
    "  - **stochastic_text_generation**: Randomly generates text by selecting words from the input.\n",
    "  - **masking**: Masks random words in the text.\n",
    "  - **correct_grammar**: Uses a T5 model to correct grammar.\n",
    "  - **generation**: Uses a T5 model to generate text based on input.\n",
    "  - **combination**: Combines selected words from the input text.\n",
    "  - **conditional_text_generation**: Generates text based on input conditions using a T5 model.\n",
    "  - **bert_augmentation**: Uses BERT to augment text by masking and predicting words.\n",
    "  - **hierarchical_text_generation**: Applies multiple augmentations to generate text.\n",
    "  - **back_translation**: Translates text to another language and back for augmentation.\n",
    "\n",
    "- **Applying Augmentations**:\n",
    "  - **start_augmentation_process**: Applies various text augmentation techniques to specified text columns in a DataFrame. It iterates over the columns and applies each selected augmentation, creating new columns in the DataFrame for each augmented version of the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088ada-9c2b-4b0f-90c8-9b2ee4ab7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertModel\n",
    "from deep_translator import GoogleTranslator\n",
    "from itertools import chain\n",
    "\n",
    "# References: https://dzlab.github.io/dltips/en/pytorch/text-augmentation/\n",
    "# https://www.kaggle.com/code/nandhuelan/nlp-augmentation-bert\n",
    "# https://github.com/topics/text-augmentation?o=asc&s=forks\n",
    "class TextAugmentation:\n",
    "    \"\"\"\n",
    "    Class for performing text augmentation using various techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the TextAugmentation object.\n",
    "\n",
    "        Args:\n",
    "            config (dict): Configuration for text augmentation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        # Initialize T5 tokenizer and model\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "        # Initialize BERT tokenizer and model\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'\\[CLS\\]|\\[SEP\\]|\\[PAD\\]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def synonym_replacement(self, text):\n",
    "        \"\"\"\n",
    "        Perform synonym replacement on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for synonym replacement.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after synonym replacement.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        augmented_text = []\n",
    "        for word in words:\n",
    "            synonyms = self.get_synonyms(word)\n",
    "            if synonyms:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "                augmented_text.append(synonym)\n",
    "            else:\n",
    "                augmented_text.append(word)\n",
    "        return \" \".join(augmented_text)\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        \"\"\"\n",
    "        Get synonyms for a word using WordNet.\n",
    "\n",
    "        Args:\n",
    "            word (str): Input word to find synonyms for.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of synonyms for the input word.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            synonyms = nltk.corpus.wordnet.synsets(word)\n",
    "            return set(chain(*[syn.lemma_names() for syn in synonyms]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting synonyms: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def add_noise(self, text, noise_level='low'):\n",
    "        \"\"\"\n",
    "        Add noise to the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to add noise to.\n",
    "            noise_level (str): Level of noise to add ('low', 'medium', 'high').\n",
    "\n",
    "        Returns:\n",
    "            str: Text after adding noise.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        noisy_text = []\n",
    "        for char in text:\n",
    "            if char.isalpha():\n",
    "                if random.random() < 0.1:  # Adjust noise probability based on level\n",
    "                    noisy_text.append(random.choice(string.ascii_letters))\n",
    "                else:\n",
    "                    noisy_text.append(char)\n",
    "            else:\n",
    "                noisy_text.append(char)\n",
    "        return ''.join(noisy_text)\n",
    "\n",
    "    def random_insertion(self, text, num_inserts=3):\n",
    "        \"\"\"\n",
    "        Perform random insertion on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for random insertion.\n",
    "            num_inserts (int): Number of random insertions to perform.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after random insertion.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        augmented_text = words[:]\n",
    "        for _ in range(num_inserts):\n",
    "            random_index = random.randint(0, len(words) - 1)\n",
    "            random_letter = random.choice(string.ascii_lowercase)\n",
    "            augmented_text.insert(random_index, random_letter)\n",
    "        return \" \".join(augmented_text)\n",
    "\n",
    "    def random_deletion(self, text, probability=0.2):\n",
    "        \"\"\"\n",
    "        Perform random deletion on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for random deletion.\n",
    "            probability (float): Probability of deleting each word.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after random deletion.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        augmented_text = []\n",
    "        for word in words:\n",
    "            if random.random() > probability:\n",
    "                augmented_text.append(word)\n",
    "        return \" \".join(augmented_text)\n",
    "\n",
    "    def random_swap(self, text, num_swaps=2):\n",
    "        \"\"\"\n",
    "        Perform random swap on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for random swap.\n",
    "            num_swaps (int): Number of random swaps to perform.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after random swap.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        augmented_text = words[:]\n",
    "        for _ in range(num_swaps):\n",
    "            if len(augmented_text) > 1:\n",
    "                random_index1 = random.randint(0, len(augmented_text) - 1)\n",
    "                random_index2 = random.randint(0, len(augmented_text) - 1)\n",
    "                augmented_text[random_index1], augmented_text[random_index2] = augmented_text[random_index2], \\\n",
    "                augmented_text[random_index1]\n",
    "        return \" \".join(augmented_text)\n",
    "\n",
    "    def paraphrasing(self, text):\n",
    "        \"\"\"\n",
    "        Perform paraphrasing on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for paraphrasing.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after paraphrasing.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        augmented_text = []\n",
    "        for word in words:\n",
    "            synonyms = self.get_synonyms(word)\n",
    "            if synonyms:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "                augmented_text.append(synonym)\n",
    "            else:\n",
    "                augmented_text.append(word)\n",
    "        return \" \".join(augmented_text)\n",
    "\n",
    "    def style_transfer_nmt(self, text, target_style='en'):\n",
    "        \"\"\"\n",
    "        Perform style transfer using neural machine translation (NMT).\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for style transfer.\n",
    "            target_style (str, optional): Target style for style transfer. Defaults to 'en'.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after style transfer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            transformed_text = text.upper() if target_style == 'en' else text.lower()\n",
    "            return transformed_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during style transfer: {e}\")\n",
    "            return text\n",
    "\n",
    "    def stochastic_text_generation(self, text):\n",
    "        \"\"\"\n",
    "        Perform stochastic text generation on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for stochastic text generation.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = text.split()\n",
    "        generated_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if random.random() < 0.5:\n",
    "                generated_words.append(word)\n",
    "            else:\n",
    "                random_word = random.choice(words)\n",
    "                generated_words.append(random_word)\n",
    "\n",
    "        generated_text = \" \".join(generated_words)\n",
    "        return generated_text\n",
    "\n",
    "    def masking(self, text):\n",
    "        \"\"\"\n",
    "        Perform masking on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for masking.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after masking.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        words = text.split()\n",
    "        masked_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if random.random() < 0.3:\n",
    "                masked_words.append(\"[MASK]\")\n",
    "            else:\n",
    "                masked_words.append(word)\n",
    "\n",
    "        masked_text = \" \".join(masked_words)\n",
    "        return masked_text\n",
    "\n",
    "    def correct_grammar(self, text):\n",
    "        \"\"\"\n",
    "        Correct grammar of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for grammar correction.\n",
    "\n",
    "        Returns:\n",
    "            str: Text after grammar correction.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = \"correct grammar: \" + text\n",
    "            input_ids = self.t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            outputs = self.t5_model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "            corrected_text = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return corrected_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during grammar correction: {e}\")\n",
    "            return text\n",
    "\n",
    "    def generation(self, sentence):\n",
    "        \"\"\"\n",
    "        Generate text based on the input sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence for text generation.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_ids = self.t5_tokenizer.encode(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            outputs = self.t5_model.generate(input_ids, max_length=512, num_beams=5, temperature=0.7,\n",
    "                                             early_stopping=True)\n",
    "            generated_text = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during text generation: {e}\")\n",
    "            return sentence\n",
    "\n",
    "    def combination(self, text):\n",
    "        \"\"\"\n",
    "        Perform combination of selected words from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for combination.\n",
    "\n",
    "        Returns:\n",
    "            str: Combined text.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        try:\n",
    "            words = text.split()\n",
    "\n",
    "            #             # Determine the number of words to select\n",
    "            num_words_to_select = random.randint(1, len(words))\n",
    "\n",
    "            # Select the words\n",
    "            selected_words = words[:num_words_to_select]\n",
    "\n",
    "            # Combine the selected words into a sentence\n",
    "            combined_sentence = ' '.join(selected_words)\n",
    "\n",
    "            return combined_sentence\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentence combination: {e}\")\n",
    "            return text\n",
    "\n",
    "    def conditional_text_generation(self, text):\n",
    "        \"\"\"\n",
    "        Perform conditional text generation on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for conditional text generation.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        try:\n",
    "            input_ids = self.t5_tokenizer.encode(\"generate response for: \" + text, return_tensors=\"pt\", max_length=512,\n",
    "                                                 truncation=True)\n",
    "            outputs = self.t5_model.generate(input_ids, max_length=150, num_beams=5, temperature=0.7, do_sample=True,\n",
    "                                             early_stopping=True)\n",
    "            generated_text = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during conditional text generation: {e}\")\n",
    "            return text\n",
    "\n",
    "    def bert_augmentation(self, text):\n",
    "        \"\"\"\n",
    "        Perform BERT-based text augmentation based on input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Augmented text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize the input text\n",
    "            tokens = self.bert_tokenizer.tokenize(text)\n",
    "\n",
    "            # Randomly choose a token to mask\n",
    "            masked_index = random.randint(0, len(tokens) - 1)\n",
    "            tokens[masked_index] = '[MASK]'\n",
    "\n",
    "            # Convert tokens to tensor\n",
    "            indexed_tokens = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "            # Get hidden states from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(tokens_tensor)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            # Predict the token from hidden states\n",
    "            hidden_state = hidden_states[0, masked_index]\n",
    "            predicted_token_id = torch.argmax(hidden_state).item()\n",
    "            predicted_token = self.bert_tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "            # Replace the masked token with the predicted token\n",
    "            tokens[masked_index] = predicted_token\n",
    "\n",
    "            # Reconstruct the augmented text\n",
    "            augmented_text = self.bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            return augmented_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during BERT-based text augmentation: {e}\")\n",
    "            return text\n",
    "\n",
    "    def hierarchical_text_generation(self, text):\n",
    "        \"\"\"\n",
    "        Perform hierarchical text generation on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for hierarchical text generation.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text.\n",
    "        \"\"\"\n",
    "        text = self.synonym_replacement(text)\n",
    "        text = self.paraphrasing(text)\n",
    "        text = self.stochastic_text_generation(text)\n",
    "        return text\n",
    "\n",
    "    def back_translation(self, text, source_lang='en', intermediate_lang='es'):\n",
    "        \"\"\"\n",
    "        Perform back translation on a given text.\n",
    "\n",
    "        :param text: The original text to be back-translated.\n",
    "        :param source_lang: The source language of the original text (default is 'en' for English).\n",
    "        :param intermediate_lang: The intermediate language to translate to and then back from (default is 'es' for Spanish).\n",
    "        :return: The back-translated text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Translate the text to the intermediate language\n",
    "            translated_text = GoogleTranslator(source=source_lang, target=intermediate_lang).translate(text)\n",
    "\n",
    "            # Translate the text back to the original language\n",
    "            back_translated_text = GoogleTranslator(source=intermediate_lang, target=source_lang).translate(\n",
    "                translated_text)\n",
    "            return back_translated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during back translation: {e}\")\n",
    "            return text\n",
    "\n",
    "    def start_augmentation_process(self, df1, text_columns):\n",
    "        \"\"\"\n",
    "        Start the text augmentation process on the given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df1 (pd.DataFrame): The input DataFrame containing text data.\n",
    "            text_columns (list): List of text column names to apply augmentation to.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The input DataFrame with augmented text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in text_columns:\n",
    "                df1[f'{col}_synonym_replacement'] = df1['Sentence'].apply(self.synonym_replacement)\n",
    "                df1[f'{col}_add_noise'] = df1['Sentence'].apply(self.add_noise)\n",
    "                df1[f'{col}_random_insertion'] = df1['Sentence'].apply(self.random_insertion)\n",
    "                df1[f'{col}_random_deletion'] = df1['Sentence'].apply(self.random_deletion)\n",
    "                df1[f'{col}_random_swap'] = df1['Sentence'].apply(self.random_swap)\n",
    "                df1[f'{col}_paraphrasing'] = df1['Sentence'].apply(self.paraphrasing)\n",
    "                df1[f'{col}_style_transfer_nmt'] = df1['Sentence'].apply(\n",
    "                    lambda x: self.style_transfer_nmt(x, target_style='en'))\n",
    "                df1[f'{col}_stochastic_text_generation'] = df1['Sentence'].apply(self.stochastic_text_generation)\n",
    "                df1[f'{col}_hierarchical_text_generation'] = df1['Sentence'].apply(self.hierarchical_text_generation)\n",
    "                df1[f'{col}_masking'] = df1['Sentence'].apply(self.masking)\n",
    "                df1[f'{col}_correct_grammar'] = df1['Sentence'].apply(self.correct_grammar)\n",
    "                df1[f'{col}_generation'] = df1['Sentence'].apply(self.generation)\n",
    "                df1[f'{col}_conditional_text_generation'] = df1['Sentence'].apply(self.conditional_text_generation)\n",
    "                df1[f'{col}_bert_augmentation'] = df1['Sentence'].apply(self.bert_augmentation)\n",
    "                df1[f'{col}_combination'] = df1['Sentence'].apply(self.combination)\n",
    "                df1[f'{col}_back_translation'] = (df1['Sentence']\n",
    "                                                  .apply(self.back_translation))  # Applying back_translation\n",
    "        except Exception as e:\n",
    "            print(f\"Error during augmentation process: {e}\")\n",
    "        finally:\n",
    "            return df1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
